<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://larocaroja.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://larocaroja.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-15T08:37:16+00:00</updated><id>https://larocaroja.github.io/feed.xml</id><title type="html">Hongseok Oh</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Microphone Conversion: Mitigating Device Variability in Sound Event Classification</title><link href="https://larocaroja.github.io/blog/2024/icassp-mic-conversion/" rel="alternate" type="text/html" title="Microphone Conversion: Mitigating Device Variability in Sound Event Classification" /><published>2024-01-03T15:58:00+00:00</published><updated>2024-01-03T15:58:00+00:00</updated><id>https://larocaroja.github.io/blog/2024/icassp-mic-conversion</id><content type="html" xml:base="https://larocaroja.github.io/blog/2024/icassp-mic-conversion/"><![CDATA[<p>In this post, I am going to introduce our work accepted to <a href="https://2024.ieeeicassp.org">2024 IEEE ICASSP</a>. You can access the full paper <a href="https://arxiv.org/pdf/2401.06913.pdf">here</a>.</p>

<h2 id="exploring-the-world-of-sound-recognition">Exploring the World of Sound Recognition</h2>
<h3 id="sound-event-classification-and-the-challenge-of-device-variability">Sound Event Classification and the Challenge of Device Variability</h3>
<p>Sound Event Classification (SEC) is a fascinating area of technology that aims to identify different types of sounds, like speech, music, and environmental noises, using advanced signal processing and machine learning techniques. Sound Event Classification (SEC) powers popular technologies like Apple’s Sound Recognition, Amazon’s Alexa, and Google Home, enabling them to identify sounds from speech to environmental noises. Despite its broad applications, SEC faces challenges, particularly when audio is recorded on different devices, leading to performance issues. These variations, often imperceptible to human ears, can drastically affect the performance of SEC systems.</p>

<div class="fake-img l-body">
  <p><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/apple-sound-recognition.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
</p>
</div>
<div class="caption">
  Apple's Sound Recognition &lt;https://www.youtube.com/watch?v=Db9Xsw5Aa5w&gt;
</div>

<h3 id="our-innovative-approach-to-enhance-reliability">Our Innovative Approach to Enhance Reliability</h3>
<p>Traditional solutions, such as data augmentation, have been limited, often relying on synthetic data which doesn’t fully capture real-world complexities. Furthermore, most initiatives, including those like <a href="https://dcase.community">the DCASE Challenge</a>, have their limitations, primarily due to their reliance on synthetic evaluation data. This limitation raises questions about the thoroughness and real-world applicability of their evaluations. Recognizing these challenges, our research addresses these challenges, aiming to enhance SEC systems for more accurate and reliable use in everyday technologies.</p>

<hr />

<h2 id="developing-a-new-technique-and-dataset">Developing a New Technique and Dataset</h2>
<h3 id="how-we-created-a-unique-sound-dataset">How We Created a Unique Sound Dataset</h3>
<p>In our journey to enhance sound event classification, we crafted the ‘Deeply Device Dataset,’ a diverse collection capturing 75 sound classes. This dataset, a mix of direct recordings and recordings of the playback of anechoics samples from <a href="https://www.openslr.org/13/">the RWCP Sound Scene Database</a>, includes a variety of sounds from daily life noises and human interactions to musical instruments. Key to our approach was the use of 18 different recording devices, encompassing a range of smartphones, laptops, and specialized microphones, each adding its unique acoustic signature to the dataset.</p>

<p>The recording process unfolded in an anechoic chamber, an environment meticulously designed to eliminate external noise and echo, ensuring pristine audio capture. This setup allowed us to systematically record each sound class, with devices positioned to optimally capture the emitted sounds. The result is two subsets: a comprehensive one with all sound classes and devices, and a smaller, focused subset. Each sound event in these subsets was aligned and annotated with precision, offering a rich resource for analyzing how different devices impact sound event classification.</p>

<p>A comprehensive list of classes is provided as follows:</p>

<ul>
  <li>Directly-produced (25)
    <ul>
      <li>Sound of things (11): cell phone alarm, cell phone vibrating, hair dryer, fan, dish clanking, typing keyboard,clicking mouse, pouring water, knock, rustling, impulse</li>
      <li>Human sound (7): speech_1 (‘help me’ in Korean), speech_2 (‘save me’ in Korean), coughing, clapping, finger snap, whistle, throat clearing</li>
      <li>Musical instrument (7): ukulele, guitar, tambourine, hand drum, castanets, triangle, hand cymbals</li>
    </ul>
  </li>
  <li>RWCP-SSD (50)
    <ul>
      <li>Class 1 (17): bank, bottle, bowl, candybwl, case, cherry, china, coffcan, colacan, cup, dice, magno, metal, pan, teak, trashbox, wood</li>
      <li>Class 2 (10): aircap, cap, clap, claps, file, pump, sandpp, saw, snap, sticks</li>
      <li>Class 3 (23): bells, book, buzzer, castanet, clock1, clock2, coffmill, coin, coins, cymbals, doorlock, dryer, horn, maracas, padlock, phone , pipong, ring, shaver, stapler, string, toy, whistle</li>
    </ul>
  </li>
</ul>

<div class="fake-img l-body">
  <p><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/anechoic-chamber.jpg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
</p>
</div>
<div class="caption">
  The anechoic chamber used for the experiments
</div>

<h3 id="simplifying-our-method-for-better-sound-recognition">Simplifying Our Method for Better Sound Recognition</h3>
<p>In our pursuit to enhance sound event classification (SEC), we introduced ‘Microphone Conversion’. This innovative technique aims to bridge the gap between different recording devices by transforming the spectrograms from one device to mimic those of another. At the heart of this technique is a mapping function, designed to convert the spectrogram data from a source device ($X_A$) into a form that statistically resembles the target device ($X_B$).</p>

<div class="fake-img l-body">
  <p><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/conversion-visualization.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
</p>
</div>
<div class="caption">
  Spectrograms of the real/generated coughing sounds are presented in the top and bottom rows, respectively. The generated ones are produced by corresponding Microphone Conversion networks using a real input of iPhone 14.
</div>

<p>To achieve this, we employed CycleGAN<d-cite key="zhu2017unpaired"></d-cite>, a groundbreaking framework in the realm of unsupervised image-to-image translation. CycleGAN is adept at learning mappings between two unpaired data domains, making it ideal for our purpose. Built on Generative Adversarial Networks (GANs), it involves two generators, $F$ and $G$, and two discriminators, $D_A$ and $D_B$. These components work in tandem: while the generators map data bijectively between the domains, the discriminators aim to distinguish real images from the converted ones.</p>

<p>The effectiveness of CycleGAN in our context lies in its dual-loss setup. The adversarial loss pushes the generators to produce outputs that are indistinguishable from the target domain, ensuring authenticity. Meanwhile, the cycle-consistency loss guarantees that the generated images, once reverse-mapped, closely resemble their original forms. This balance ensures that while the style of the spectrograms adapts to the target domain, their content remains intact.</p>

\[\mathcal{L}_{adv}(F, D_B, X_A, X_B) = \\
\mathbb{E}_{X_A{\sim}p(x_a)}\left[{\log}(1-D_B(F(X_A)))\right] + \mathbb{E}_{X_B{\sim}p(x_b)}\left[{\log}D_B(X_B)\right]\]

\[\mathcal{L}_{cycle}(F, G, X_A, X_B) = 
\mathbb{E}_{X_A{\sim}p(x_a)}[||{G(F(X_A)) - X_A}||_{1}] + \mathbb{E}_{X_B{\sim}p(x_b)}[||{F(G(X_B)) - X_B}||_{1}]\]

<p>For the network architecture, we adapted the CycleGAN implementation directly from the original author’s code. Our generator network consists of two up- and down-sampling layers, along with nine residual blocks with instance normalization, with the omission of the hyperbolic tangent layer. The discriminator, a 16x16 PatchGAN with instance normalization, was chosen for its effectiveness in minimizing blurriness and ensuring better convergence in the output.</p>

<p>Through this approach, we were able to effectively ‘translate’ audio data across devices, preserving the integrity of the sound events while adapting to the unique acoustic characteristics of various recording devices.</p>

<hr />

<h2 id="experiments-and-results">Experiments and Results</h2>
<h3 id="comparative-analysis-our-method-vs-traditional-approaches">Comparative Analysis: Our Method vs. Traditional Approaches</h3>
<p>In our research, we demonstrated how our proposed method can benefit SEC systems in the face of device variability. We also conducted a thorough examination of the leading methods from recent DCASE Challenges, which are designed to mitigate device variability in sound event classification (SEC) systems, and compare their results with ours. The experiments were conducted using audio samples recorded from 7 different recording devices: iPhone 14, Galaxy S22, iPad 7, Galaxy Tab A8, Apple Watch SE, Macbook Pro(’20), and LG Gram(’20).</p>

<p>We evaluated techniques like Freq-MixStyle<d-cite key="Schmid2022"></d-cite> and Residual Normalization<d-cite key="Kim2021b"></d-cite>, which leverage frequency-wise statistics for improved generalization. Additionally, we explored an extended version known as Relaxed Instance Frequency-wise Normalization (RFN)<d-cite key="kim22_interspeech"></d-cite> applied across five bottleneck blocks, and FilterAugment<d-cite key="Nam2021"></d-cite>, which adjusts spectrograms using optimally generated linear filters. Our testing also included standard data augmentations such as Gaussian noise, room impulse response, and pitch shift, along with advanced methods like SpecAugment and MixUp, each applied with a probability of 0.5, except for RFN.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>S</th>
      <th>T1</th>
      <th>T2</th>
      <th>T3</th>
      <th>T4</th>
      <th>T5</th>
      <th>T6</th>
      <th>Overall (- S)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>0.982</td>
      <td>0.409</td>
      <td>0.709</td>
      <td>0.248</td>
      <td>0.471</td>
      <td>0.687</td>
      <td>0.491</td>
      <td>0.503 ± 0.167</td>
    </tr>
    <tr>
      <td>Gaussian Noise</td>
      <td>0.983</td>
      <td>0.708</td>
      <td>0.918</td>
      <td>0.576</td>
      <td>0.565</td>
      <td>0.780</td>
      <td>0.683</td>
      <td>0.705 ± 0.127</td>
    </tr>
    <tr>
      <td>Reverberation</td>
      <td>0.980</td>
      <td>0.895</td>
      <td>0.852</td>
      <td>0.539</td>
      <td>0.832</td>
      <td>0.736</td>
      <td>0.360</td>
      <td>0.702 ± 0.202</td>
    </tr>
    <tr>
      <td>Pitch Shift</td>
      <td>0.981</td>
      <td>0.471</td>
      <td>0.744</td>
      <td>0.221</td>
      <td>0.658</td>
      <td>0.648</td>
      <td>0.442</td>
      <td>0.531 ± 0.183</td>
    </tr>
    <tr>
      <td>SpecAugment</td>
      <td><strong>0.985</strong></td>
      <td>0.372</td>
      <td>0.762</td>
      <td>0.214</td>
      <td>0.363</td>
      <td>0.634</td>
      <td>0.324</td>
      <td>0.445 ± 0.199</td>
    </tr>
    <tr>
      <td>MixUp</td>
      <td>0.983</td>
      <td>0.336</td>
      <td>0.677</td>
      <td>0.213</td>
      <td>0.449</td>
      <td>0.656</td>
      <td>0.387</td>
      <td>0.453 ± 0.175</td>
    </tr>
    <tr>
      <td>FilterAugment</td>
      <td>0.981</td>
      <td>0.964</td>
      <td>0.891</td>
      <td>0.586</td>
      <td>0.874</td>
      <td>0.794</td>
      <td>0.642</td>
      <td>0.792 ± 0.143</td>
    </tr>
    <tr>
      <td>Freq-MixStyle</td>
      <td>0.974</td>
      <td>0.839</td>
      <td>0.879</td>
      <td>0.795</td>
      <td>0.902</td>
      <td><strong>0.885</strong></td>
      <td>0.832</td>
      <td>0.855 ± 0.038</td>
    </tr>
    <tr>
      <td>RFN</td>
      <td>0.980</td>
      <td>0.919</td>
      <td>0.909</td>
      <td>0.742</td>
      <td>0.907</td>
      <td>0.829</td>
      <td>0.614</td>
      <td>0.820 ± 0.116</td>
    </tr>
    <tr>
      <td>MC-100-Gen</td>
      <td>0.981</td>
      <td>0.958</td>
      <td><strong>0.912</strong></td>
      <td>0.894</td>
      <td>0.899</td>
      <td>0.831</td>
      <td>0.852</td>
      <td>0.891 ± 0.043</td>
    </tr>
    <tr>
      <td>MC-200-Gen</td>
      <td>0.982</td>
      <td><strong>0.969</strong></td>
      <td>0.909</td>
      <td><strong>0.903</strong></td>
      <td><strong>0.912</strong></td>
      <td>0.859</td>
      <td><strong>0.887</strong></td>
      <td><strong>0.907 ± 0.035</strong></td>
    </tr>
    <tr>
      <td>MC-100-Adapt (p=0.5)</td>
      <td>-</td>
      <td>0.956</td>
      <td>0.905</td>
      <td>0.904</td>
      <td>0.880</td>
      <td>0.902</td>
      <td>0.890</td>
      <td>0.906 ± 0.025</td>
    </tr>
    <tr>
      <td>MC-100-Adapt (p=1.0)</td>
      <td>-</td>
      <td>0.891</td>
      <td>0.906</td>
      <td>0.855</td>
      <td>0.834</td>
      <td>0.811</td>
      <td>0.808</td>
      <td>0.851 ± 0.039</td>
    </tr>
    <tr>
      <td>MC-200-Adapt (p=0.5)</td>
      <td>-</td>
      <td>0.965</td>
      <td><strong>0.922</strong></td>
      <td><strong>0.906</strong></td>
      <td>0.910</td>
      <td><strong>0.908</strong></td>
      <td><strong>0.907</strong></td>
      <td><strong>0.920 ± 0.022</strong></td>
    </tr>
    <tr>
      <td>MC-200-Adapt (p=1.0)</td>
      <td>-</td>
      <td>0.935</td>
      <td>0.917</td>
      <td>0.864</td>
      <td>0.873</td>
      <td>0.880</td>
      <td>0.848</td>
      <td>0.886 ± 0.032</td>
    </tr>
    <tr>
      <td>Real</td>
      <td>0.983</td>
      <td>0.982</td>
      <td>0.972</td>
      <td>0.985</td>
      <td>0.979</td>
      <td>0.983</td>
      <td>0.986</td>
      <td>0.981 ± 0.005</td>
    </tr>
  </tbody>
</table>

<p>Our evaluation, as detailed in the above table, highlighted significant performance vulnerabilities in SEC systems when dealing with heterogeneous recording devices. A striking example was observed in the case of Galaxy Tab A8, where there was a dramatic performance drop of up to 73.7% compared to the baseline ‘Real’ metric, an ideal case where training and inference domain matches. Conversely, models showed better performance on devices like iPad 7 and Macbook Pro(’20), which share more similarities with the training data, as evidenced by their proximity in the t-SNE feature space, with a more modest performance drop of 26.3% and 29.5%, respectively. Traditional data augmentation methods offered limited improvement.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_all_5.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_all_inverse_30.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_whistle_30.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_whistle_inverse_30.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    2D t-SNE visualization of intermediate embeddings of the baseline on the development data. The left column illustrates the full development set and whistle sound samples drawn from it, respectively. The right column are sound samples converted to iPhone 14 using the date from the left.
</div>

<p>Among the more specialized recent approaches, Freq-MixStyle emerged as the most effective, achieving an average F1 score of 85.5% and demonstrating consistent performance with a narrow 3.8% confidence interval. However, our Microphone Conversion technique, MC-200-Gen, trained for 200 epochs, outshone these methods with an impressive average F1 score of 90.7% and minimal variability (3.5%). In scenarios requiring adaptation, SEC models employing adaptation strategies generally outperformed MC-200-Adapt (p=0.5) on 4 out of 6 target devices, showing a 1.3% gain in overall performance. Remarkably, they nearly matched the ‘Real’ performance metric when trained on a diverse set of devices.</p>

<p>Our findings underscore the effectiveness of our Microphone Conversion approach, particularly its capability to enhance both generalization and adaptability of SEC systems. The promising results also suggest the potential for further optimization with extended training durations, paving the way for more robust and reliable SEC systems in the face of device variability.</p>

<h3 id="implications-and-impact-of-our-findings">Implications and Impact of Our Findings</h3>
<p>In addressing the challenge of device variability in sound event classification (SEC) systems, our study has made significant strides with far-reaching implications. The development of a specialized sound event dataset, recorded across a variety of real-world devices in an anechoic chamber, marks a substantial advancement in the field. This dataset not only provides a more realistic and robust basis for training and testing SEC systems but also sets a new standard for future research in this area.</p>

<p>Our introduction of the Microphone Conversion method has been a game-changer in improving the performance of SEC systems. By significantly outperforming recent approaches in both generalization and adaptation tasks, this method showcases the potential of machine learning techniques in overcoming complex challenges in audio signal processing. The ability of our method to adapt to various recording devices enhances the versatility and reliability of SEC systems, paving the way for their broader application in diverse real-world scenarios. This includes improved accuracy in voice-activated devices, enhanced sound recognition in security systems, and more effective environmental sound analysis in smart city infrastructure.</p>

<p>However, our findings also highlight an area for future exploration. The CycleGAN component of our solution, while effective, is currently limited by its assumption of a one-to-one domain mapping. This necessitates separate models for each domain pair, which can be resource-intensive and less scalable. Exploring the integration of impulse response characteristics with CycleGAN presents an exciting opportunity for more versatile domain mapping. Such advancements could lead to even more sophisticated and adaptable SEC systems, capable of handling a wider array of acoustic environments with fewer model constraints.</p>

<p>In summary, our findings not only demonstrate a significant improvement in handling device variability in SEC but also open up new avenues for research and development. The implications of this work extend beyond academic research, suggesting practical applications in various industries where sound recognition plays a crucial role. As technology continues to evolve, the integration of our methods could lead to more intelligent, adaptable, and efficient sound recognition systems, fundamentally transforming how we interact with and interpret the sounds in our environment.</p>

<hr />

<h2 id="citation">Citation</h2>
<p>Myeonghoon Ryu<sup>*</sup>, Hongseok Oh<sup>*</sup>, Han Park, Suji Lee. “Microphone Conversion: Mitigating Device Variability in Sound Event Classification”, in <em>2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2024</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">icassp2024mic_conversion</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Myeonghoon Ryu and Hongseok Oh and Han Park and Suji Lee}</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Microphone Conversion: Mitigating Device Variability in Sound Event Classification}</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
    <span class="na">organization</span><span class="p">=</span><span class="s">{IEEE}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Hongseok Oh</name></author><category term="research" /><category term="deep-learning" /><category term="generative-AI" /><category term="research" /><category term="audio-event-classification" /><summary type="html"><![CDATA[[ICASSP 2024] A generative method to tackle domain mismatch problem in sound event classifcation systems]]></summary></entry></feed>