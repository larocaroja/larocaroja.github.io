<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Microphone Conversion: Mitigating Device Variability in Sound Event Classification | Hongseok Oh</title>
    <meta name="author" content="Hongseok  Oh">
    <meta name="description" content="[ICASSP 2024] A generative method to tackle domain mismatch problem in sound event classifcation systems">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.3/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://larocaroja.github.io/blog/2024/icassp-mic-conversion/">

    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Microphone Conversion: Mitigating Device Variability in Sound Event Classification",
      "description": "[ICASSP 2024] A generative method to tackle domain mismatch problem in sound event classifcation systems",
      "published": "January 3, 2024",
      "authors": [
        {
          "author": "Hongseok Oh",
          "authorURL": "",
          "affiliations": [
            {
              "name": "University of Calfironia, San Diego",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/">Hongseok Oh</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Microphone Conversion: Mitigating Device Variability in Sound Event Classification</h1>
        <p>[ICASSP 2024] A generative method to tackle domain mismatch problem in sound event classifcation systems</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#exploring-the-world-of-sound-recognition">Exploring the World of Sound Recognition</a></div>
            <div><a href="#developing-a-new-technique-and-dataset">Developing a New Technique and Dataset</a></div>
            <div><a href="#experiments-and-results">Experiments and Results</a></div>
            
          </nav>
        </d-contents>

        <h2 id="exploring-the-world-of-sound-recognition">Exploring the World of Sound Recognition</h2>
<h3 id="sound-event-classification-and-the-challenge-of-device-variability">Sound Event Classification and the Challenge of Device Variability</h3>
<p>Sound Event Classification (SEC) is a fascinating area of technology that aims to identify different types of sounds, like speech, music, and environmental noises, using advanced signal processing and machine learning techniques. Sound Event Classification (SEC) powers popular technologies like Apple’s Sound Recognition, Amazon’s Alexa, and Google Home, enabling them to identify sounds from speech to environmental noises. Despite its broad applications, SEC faces challenges, particularly when audio is recorded on different devices, leading to performance issues. These variations, often imperceptible to human ears, can drastically affect the performance of SEC systems.</p>

<div class="fake-img l-body">
  <p><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/apple-sound-recognition.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</p>
</div>
<div class="caption">
  Apple's Sound Recognition &lt;https://www.youtube.com/watch?v=Db9Xsw5Aa5w&gt;
</div>

<h3 id="our-innovative-approach-to-enhance-reliability">Our Innovative Approach to Enhance Reliability</h3>
<p>Traditional solutions, such as data augmentation, have been limited, often relying on synthetic data which doesn’t fully capture real-world complexities. Furthermore, most initiatives, including those like <a href="https://dcase.community" rel="external nofollow noopener" target="_blank">the DCASE Challenge</a>, have their limitations, primarily due to their reliance on synthetic evaluation data. This limitation raises questions about the thoroughness and real-world applicability of their evaluations. Recognizing these challenges, our research addresses these challenges, aiming to enhance SEC systems for more accurate and reliable use in everyday technologies.</p>

<hr>

<h2 id="developing-a-new-technique-and-dataset">Developing a New Technique and Dataset</h2>
<h3 id="how-we-created-a-unique-sound-dataset">How We Created a Unique Sound Dataset</h3>
<p>In our journey to enhance sound event classification, we crafted the ‘Deeply Device Dataset,’ a diverse collection capturing 75 sound classes. This dataset, a mix of direct recordings and recordings of the playback of anechoics samples from <a href="https://www.openslr.org/13/" rel="external nofollow noopener" target="_blank">the RWCP Sound Scene Database</a>, includes a variety of sounds from daily life noises and human interactions to musical instruments. Key to our approach was the use of 18 different recording devices, encompassing a range of smartphones, laptops, and specialized microphones, each adding its unique acoustic signature to the dataset.</p>

<p>The recording process unfolded in an anechoic chamber, an environment meticulously designed to eliminate external noise and echo, ensuring pristine audio capture. This setup allowed us to systematically record each sound class, with devices positioned to optimally capture the emitted sounds. The result is two subsets: a comprehensive one with all sound classes and devices, and a smaller, focused subset. Each sound event in these subsets was aligned and annotated with precision, offering a rich resource for analyzing how different devices impact sound event classification.</p>

<p>A comprehensive list of classes is provided as follows:</p>

<ul>
  <li>Directly-produced (25)
    <ul>
      <li>Sound of things (11): cell phone alarm, cell phone vibrating, hair dryer, fan, dish clanking, typing keyboard,clicking mouse, pouring water, knock, rustling, impulse</li>
      <li>Human sound (7): speech_1 (‘help me’ in Korean), speech_2 (‘save me’ in Korean), coughing, clapping, finger snap, whistle, throat clearing</li>
      <li>Musical instrument (7): ukulele, guitar, tambourine, hand drum, castanets, triangle, hand cymbals</li>
    </ul>
  </li>
  <li>RWCP-SSD (50)
    <ul>
      <li>Class 1 (17): bank, bottle, bowl, candybwl, case, cherry, china, coffcan, colacan, cup, dice, magno, metal, pan, teak, trashbox, wood</li>
      <li>Class 2 (10): aircap, cap, clap, claps, file, pump, sandpp, saw, snap, sticks</li>
      <li>Class 3 (23): bells, book, buzzer, castanet, clock1, clock2, coffmill, coin, coins, cymbals, doorlock, dryer, horn, maracas, padlock, phone , pipong, ring, shaver, stapler, string, toy, whistle</li>
    </ul>
  </li>
</ul>

<div class="fake-img l-body">
  <p><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/anechoic-chamber.jpg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</p>
</div>
<div class="caption">
  The anechoic chamber used for the experiments
</div>

<h3 id="simplifying-our-method-for-better-sound-recognition">Simplifying Our Method for Better Sound Recognition</h3>
<p>In our pursuit to enhance sound event classification (SEC), we introduced ‘Microphone Conversion’. This innovative technique aims to bridge the gap between different recording devices by transforming the spectrograms from one device to mimic those of another. At the heart of this technique is a mapping function, designed to convert the spectrogram data from a source device ($X_A$) into a form that statistically resembles the target device ($X_B$).</p>

<div class="fake-img l-body">
  <p><figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/conversion-visualization.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</p>
</div>
<div class="caption">
  Spectrograms of the real/generated coughing sounds are presented in the top and bottom rows, respectively. The generated ones are produced by corresponding Microphone Conversion networks using a real input of iPhone 14.
</div>

<p>To achieve this, we employed CycleGAN<d-cite key="zhu2017unpaired"></d-cite>, a groundbreaking framework in the realm of unsupervised image-to-image translation. CycleGAN is adept at learning mappings between two unpaired data domains, making it ideal for our purpose. Built on Generative Adversarial Networks (GANs), it involves two generators, $F$ and $G$, and two discriminators, $D_A$ and $D_B$. These components work in tandem: while the generators map data bijectively between the domains, the discriminators aim to distinguish real images from the converted ones.</p>

<p>The effectiveness of CycleGAN in our context lies in its dual-loss setup. The adversarial loss pushes the generators to produce outputs that are indistinguishable from the target domain, ensuring authenticity. Meanwhile, the cycle-consistency loss guarantees that the generated images, once reverse-mapped, closely resemble their original forms. This balance ensures that while the style of the spectrograms adapts to the target domain, their content remains intact.</p>

\[\mathcal{L}_{adv}(F, D_B, X_A, X_B) = \\
\mathbb{E}_{X_A{\sim}p(x_a)}\left[{\log}(1-D_B(F(X_A)))\right] + \mathbb{E}_{X_B{\sim}p(x_b)}\left[{\log}D_B(X_B)\right]\]

\[\mathcal{L}_{cycle}(F, G, X_A, X_B) = 
\mathbb{E}_{X_A{\sim}p(x_a)}[||{G(F(X_A)) - X_A}||_{1}] + \mathbb{E}_{X_B{\sim}p(x_b)}[||{F(G(X_B)) - X_B}||_{1}]\]

<p>For the network architecture, we adapted the CycleGAN implementation directly from the original author’s code. Our generator network consists of two up- and down-sampling layers, along with nine residual blocks with instance normalization, with the omission of the hyperbolic tangent layer. The discriminator, a 16x16 PatchGAN with instance normalization, was chosen for its effectiveness in minimizing blurriness and ensuring better convergence in the output.</p>

<p>Through this approach, we were able to effectively ‘translate’ audio data across devices, preserving the integrity of the sound events while adapting to the unique acoustic characteristics of various recording devices.</p>

<hr>

<h2 id="experiments-and-results">Experiments and Results</h2>
<h3 id="comparative-analysis-our-method-vs-traditional-approaches">Comparative Analysis: Our Method vs. Traditional Approaches</h3>
<p>In our research, we demonstrated how our proposed method can benefit SEC systems in the face of device variability. We also conducted a thorough examination of the leading methods from recent DCASE Challenges, which are designed to mitigate device variability in sound event classification (SEC) systems, and compare their results with ours. The experiments were conducted using audio samples recorded from 7 different recording devices: iPhone 14, Galaxy S22, iPad 7, Galaxy Tab A8, Apple Watch SE, Macbook Pro(’20), and LG Gram(’20).</p>

<p>We evaluated techniques like Freq-MixStyle<d-cite key="Schmid2022"></d-cite> and Residual Normalization<d-cite key="Kim2021b"></d-cite>, which leverage frequency-wise statistics for improved generalization. Additionally, we explored an extended version known as Relaxed Instance Frequency-wise Normalization (RFN)<d-cite key="kim22_interspeech"></d-cite> applied across five bottleneck blocks, and FilterAugment<d-cite key="Nam2021"></d-cite>, which adjusts spectrograms using optimally generated linear filters. Our testing also included standard data augmentations such as Gaussian noise, room impulse response, and pitch shift, along with advanced methods like SpecAugment and MixUp, each applied with a probability of 0.5, except for RFN.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>S</th>
      <th>T1</th>
      <th>T2</th>
      <th>T3</th>
      <th>T4</th>
      <th>T5</th>
      <th>T6</th>
      <th>Overall (- S)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline</td>
      <td>0.982</td>
      <td>0.409</td>
      <td>0.709</td>
      <td>0.248</td>
      <td>0.471</td>
      <td>0.687</td>
      <td>0.491</td>
      <td>0.503 ± 0.167</td>
    </tr>
    <tr>
      <td>Gaussian Noise</td>
      <td>0.983</td>
      <td>0.708</td>
      <td>0.918</td>
      <td>0.576</td>
      <td>0.565</td>
      <td>0.780</td>
      <td>0.683</td>
      <td>0.705 ± 0.127</td>
    </tr>
    <tr>
      <td>Reverberation</td>
      <td>0.980</td>
      <td>0.895</td>
      <td>0.852</td>
      <td>0.539</td>
      <td>0.832</td>
      <td>0.736</td>
      <td>0.360</td>
      <td>0.702 ± 0.202</td>
    </tr>
    <tr>
      <td>Pitch Shift</td>
      <td>0.981</td>
      <td>0.471</td>
      <td>0.744</td>
      <td>0.221</td>
      <td>0.658</td>
      <td>0.648</td>
      <td>0.442</td>
      <td>0.531 ± 0.183</td>
    </tr>
    <tr>
      <td>SpecAugment</td>
      <td><strong>0.985</strong></td>
      <td>0.372</td>
      <td>0.762</td>
      <td>0.214</td>
      <td>0.363</td>
      <td>0.634</td>
      <td>0.324</td>
      <td>0.445 ± 0.199</td>
    </tr>
    <tr>
      <td>MixUp</td>
      <td>0.983</td>
      <td>0.336</td>
      <td>0.677</td>
      <td>0.213</td>
      <td>0.449</td>
      <td>0.656</td>
      <td>0.387</td>
      <td>0.453 ± 0.175</td>
    </tr>
    <tr>
      <td>FilterAugment</td>
      <td>0.981</td>
      <td>0.964</td>
      <td>0.891</td>
      <td>0.586</td>
      <td>0.874</td>
      <td>0.794</td>
      <td>0.642</td>
      <td>0.792 ± 0.143</td>
    </tr>
    <tr>
      <td>Freq-MixStyle</td>
      <td>0.974</td>
      <td>0.839</td>
      <td>0.879</td>
      <td>0.795</td>
      <td>0.902</td>
      <td><strong>0.885</strong></td>
      <td>0.832</td>
      <td>0.855 ± 0.038</td>
    </tr>
    <tr>
      <td>RFN</td>
      <td>0.980</td>
      <td>0.919</td>
      <td>0.909</td>
      <td>0.742</td>
      <td>0.907</td>
      <td>0.829</td>
      <td>0.614</td>
      <td>0.820 ± 0.116</td>
    </tr>
    <tr>
      <td>MC-100-Gen</td>
      <td>0.981</td>
      <td>0.958</td>
      <td><strong>0.912</strong></td>
      <td>0.894</td>
      <td>0.899</td>
      <td>0.831</td>
      <td>0.852</td>
      <td>0.891 ± 0.043</td>
    </tr>
    <tr>
      <td>MC-200-Gen</td>
      <td>0.982</td>
      <td><strong>0.969</strong></td>
      <td>0.909</td>
      <td><strong>0.903</strong></td>
      <td><strong>0.912</strong></td>
      <td>0.859</td>
      <td><strong>0.887</strong></td>
      <td><strong>0.907 ± 0.035</strong></td>
    </tr>
    <tr>
      <td>MC-100-Adapt (p=0.5)</td>
      <td>-</td>
      <td>0.956</td>
      <td>0.905</td>
      <td>0.904</td>
      <td>0.880</td>
      <td>0.902</td>
      <td>0.890</td>
      <td>0.906 ± 0.025</td>
    </tr>
    <tr>
      <td>MC-100-Adapt (p=1.0)</td>
      <td>-</td>
      <td>0.891</td>
      <td>0.906</td>
      <td>0.855</td>
      <td>0.834</td>
      <td>0.811</td>
      <td>0.808</td>
      <td>0.851 ± 0.039</td>
    </tr>
    <tr>
      <td>MC-200-Adapt (p=0.5)</td>
      <td>-</td>
      <td>0.965</td>
      <td><strong>0.922</strong></td>
      <td><strong>0.906</strong></td>
      <td>0.910</td>
      <td><strong>0.908</strong></td>
      <td><strong>0.907</strong></td>
      <td><strong>0.920 ± 0.022</strong></td>
    </tr>
    <tr>
      <td>MC-200-Adapt (p=1.0)</td>
      <td>-</td>
      <td>0.935</td>
      <td>0.917</td>
      <td>0.864</td>
      <td>0.873</td>
      <td>0.880</td>
      <td>0.848</td>
      <td>0.886 ± 0.032</td>
    </tr>
    <tr>
      <td>Real</td>
      <td>0.983</td>
      <td>0.982</td>
      <td>0.972</td>
      <td>0.985</td>
      <td>0.979</td>
      <td>0.983</td>
      <td>0.986</td>
      <td>0.981 ± 0.005</td>
    </tr>
  </tbody>
</table>

<p>Our evaluation, as detailed in the above table, highlighted significant performance vulnerabilities in SEC systems when dealing with heterogeneous recording devices. A striking example was observed in the case of Galaxy Tab A8, where there was a dramatic performance drop of up to 73.7% compared to the baseline ‘Real’ metric, an ideal case where training and inference domain matches. Conversely, models showed better performance on devices like iPad 7 and Macbook Pro(’20), which share more similarities with the training data, as evidenced by their proximity in the t-SNE feature space, with a more modest performance drop of 26.3% and 29.5%, respectively. Traditional data augmentation methods offered limited improvement.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_all_5.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_all_inverse_30.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_whistle_30.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mic_conversion/tsne_whistle_inverse_30.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    2D t-SNE visualization of intermediate embeddings of the baseline on the development data. The left column illustrates the full development set and whistle sound samples drawn from it, respectively. The right column are sound samples converted to iPhone 14 using the date from the left.
</div>

<p>Among the more specialized recent approaches, Freq-MixStyle emerged as the most effective, achieving an average F1 score of 85.5% and demonstrating consistent performance with a narrow 3.8% confidence interval. However, our Microphone Conversion technique, MC-200-Gen, trained for 200 epochs, outshone these methods with an impressive average F1 score of 90.7% and minimal variability (3.5%). In scenarios requiring adaptation, SEC models employing adaptation strategies generally outperformed MC-200-Adapt (p=0.5) on 4 out of 6 target devices, showing a 1.3% gain in overall performance. Remarkably, they nearly matched the ‘Real’ performance metric when trained on a diverse set of devices.</p>

<p>Our findings underscore the effectiveness of our Microphone Conversion approach, particularly its capability to enhance both generalization and adaptability of SEC systems. The promising results also suggest the potential for further optimization with extended training durations, paving the way for more robust and reliable SEC systems in the face of device variability.</p>

<h3 id="implications-and-impact-of-our-findings">Implications and Impact of Our Findings</h3>
<p>In addressing the challenge of device variability in sound event classification (SEC) systems, our study has made significant strides with far-reaching implications. The development of a specialized sound event dataset, recorded across a variety of real-world devices in an anechoic chamber, marks a substantial advancement in the field. This dataset not only provides a more realistic and robust basis for training and testing SEC systems but also sets a new standard for future research in this area.</p>

<p>Our introduction of the Microphone Conversion method has been a game-changer in improving the performance of SEC systems. By significantly outperforming recent approaches in both generalization and adaptation tasks, this method showcases the potential of machine learning techniques in overcoming complex challenges in audio signal processing. The ability of our method to adapt to various recording devices enhances the versatility and reliability of SEC systems, paving the way for their broader application in diverse real-world scenarios. This includes improved accuracy in voice-activated devices, enhanced sound recognition in security systems, and more effective environmental sound analysis in smart city infrastructure.</p>

<p>However, our findings also highlight an area for future exploration. The CycleGAN component of our solution, while effective, is currently limited by its assumption of a one-to-one domain mapping. This necessitates separate models for each domain pair, which can be resource-intensive and less scalable. Exploring the integration of impulse response characteristics with CycleGAN presents an exciting opportunity for more versatile domain mapping. Such advancements could lead to even more sophisticated and adaptable SEC systems, capable of handling a wider array of acoustic environments with fewer model constraints.</p>

<p>In summary, our findings not only demonstrate a significant improvement in handling device variability in SEC but also open up new avenues for research and development. The implications of this work extend beyond academic research, suggesting practical applications in various industries where sound recognition plays a crucial role. As technology continues to evolve, the integration of our methods could lead to more intelligent, adaptable, and efficient sound recognition systems, fundamentally transforming how we interact with and interpret the sounds in our environment.</p>

<hr>

<h2 id="citation">Citation</h2>
<p>Myeonghoon Ryu<sup>*</sup>, Hongseok Oh<sup>*</sup>, Han Park, Suji Lee. “Microphone Conversion: Mitigating Device Variability in Sound Event Classification”, in <em>2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2024</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">icassp2024mic_conversion</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Myeonghoon Ryu and Hongseok Oh and Han Park and Suji Lee}</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{Microphone Conversion: Mitigating Device Variability in Sound Event Classification}</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
    <span class="na">organization</span><span class="p">=</span><span class="s">{IEEE}</span>
<span class="p">}</span>
</code></pre></div></div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/2024-01-03-icassp-mic-conversion.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "alshedivat/al-folio",
        "data-repo-id": "MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOA5PmLc4CTBt6",
        "data-mapping": "title",
        "data-strict": "1",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "bottom",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2024 Hongseok  Oh. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
